# Butterfly Anomaly Detection

## Abstract

This document describes an end-to-end pipeline for butterfly subspecies classification. The pipeline comprises the following steps:
1. Downloading and preprocessing data,
2. Training a U-Net model for wing segmentation,
3. Augmenting the wing images to obtain a balanced dataset (with at least 1,000 images per class),
4. Fine-tuning a pre-trained BiO-CLIP model by unfreezing the last two attention blocks (using a small learning rate) and training an additional classifier head (using a larger learning rate) to distinguish butterfly subspecies,
5. Estimating hybrid probabilities by comparing the top two subspecies probabilities.

## Introduction

Butterfly subspecies classification is challenging due to subtle variations in wing patterns and colors. Vision transformers are known to be capable of performing such tasks, but they require careful data preprocessing to focus on the relevant wing patterns rather than background object features. In this work, we propose a full pipeline that leverages segmentation, data augmentation, and modern fine-tuning techniques to build a robust classifier. A key aspect of the pipeline is the estimation of a hybrid probability computed by comparing the top two class probabilities.

## Data Acquisition and Preprocessing

### Downloading the Data

The dataset consists of high-resolution butterfly images along with a CSV metadata file containing subspecies labels and additional attributes. Images are downloaded from a public repository and organized into folders by class.

### Preprocessing

The preprocessing step involves:
- Resizing images to a fixed resolution (e.g., 256×256 pixels),
- Organizing data according to the metadata,
- Ensuring each butterfly subspecies is sufficiently represented.

## Wing Segmentation Using U-Net

### Model Architecture and Training

A U-Net architecture (referred to as `UNet256`) is employed for segmenting butterfly wings from the background. The network follows an encoder-decoder structure with skip connections. Notably, the U-Net was trained using wing masks generated by a combination of YOLO and SAM networks, as detailed in [Imageomics Wing Segmentation](https://github.com/Imageomics/wing-segmentation). In this process, the high-quality segmentation masks produced by the computationally intensive YOLO and SAM models serve as labels for training the much smaller and efficient U-Net. This model distillation approach enables rapid inference while retaining segmentation accuracy.

### Segmentation Process

Once trained, the U-Net model is used to extract the wing regions from butterfly images. The process involves:
- Resizing the input image,
- Applying the U-Net to generate a mask (or using the images segmented by the original YOLO+SAM approach),
- Thresholding the mask to produce a binary output,
- Removing the background based on the binary mask.

## Data Augmentation for Dataset Balancing

After segmentation, the wing images are augmented to ensure each subspecies class contains at least 1,000 images. Augmentation techniques include:
- Random rotations, flips, and affine transformations,
- Color jittering and brightness/contrast adjustments.

Both Albumentations and Torchvision pipelines are used to generate synthetic images, resulting in a balanced dataset crucial for effective model training.

## Fine-Tuning BiO-CLIP for Butterfly Classification

### BiO-CLIP Model Adaptation

We fine-tune a pre-trained BiO-CLIP model to distinguish butterfly subspecies. The adaptation strategy includes:
- Freezing most of the network and unfreezing only the last two attention blocks of the vision transformer,
- Using a small learning rate for the unfrozen layers,
- Training an additional classifier head with a larger learning rate to perform the final classification.

### Training Procedure

The fine-tuning process employs a cross-entropy loss function with the augmented wing images as inputs. A dual learning rate strategy is used:
- A low learning rate for the last two attention blocks,
- A higher learning rate for the classifier head.

This setup allows the model to adapt to the new task while leveraging the pre-trained features.

## Hybrid Probability Estimation

To robustly identify hybrid butterflies, it is insufficient to simply use a metric such as `1 - max(probability)` because in many cases of hybrids the network may assign similarly high probabilities to both parent subspecies. In such situations, relying solely on the maximum probability would lead to misclassification of these ambiguous cases as non-hybrid.

To address this, we compute a hybrid probability that considers the difference between the top two subspecies probabilities. The idea is to capture the model's uncertainty: if the two highest probabilities (corresponding to the parent subspecies) are close in value, it is more likely that the image represents a hybrid. Specifically, the method involves the following steps:
- **Sort** the predicted probabilities for all normal classes (excluding the dedicated hybrid class) in descending order.
- **Treat** the dedicated hybrid class probability (the last class) separately.
- **Define** the hybrid probability as the maximum between the dedicated hybrid probability and 
  \[
  1.0 - (\text{top1} - \text{top2})
  \]
  where \(\text{top1}\) and \(\text{top2}\) are the highest and second-highest probabilities among the normal classes.

This formulation ensures that an image is only classified as non-hybrid when there is a clear distinction between the top predicted subspecies. The following function implements this logic:

```python
def get_hybrid_prob(self, probs):
    """
    Computes the hybrid probability by comparing the top two probabilities of the normal classes.
    
    Logic:
    - Sort all but the last class in descending order.
    - The last class probability (dedicated hybrid) is treated separately.
    - Hybrid probability is defined as the maximum between the last class probability and 
      (1.0 minus the difference between the top two probabilities).
    """
    cl_probs = np.sort(probs[:, :-1], axis=1)[:, ::-1]
    last_cl = probs[:, -1]
    hybrid_probs = np.maximum(last_cl, 1.0 - (cl_probs[:, 0] - cl_probs[:, 1]))
    return hybrid_probs[0]
```

## Repository Structure
The repository is organized as follows:

```graphql

HDR-anomaly-challenge-2025/
└── butterfly anomaly challenge/
    ├── augmentation/
    │   ├── albumentation_augm.py    # Script for data augmentation using Albumentations
    │   └── augment.py               # General data augmentation script
    ├── finetune_bioclip/
    │   ├── data_utils.py            # Utility functions for data handling
    │   ├── dataset.py               # Dataset handling and preprocessing code
    │   └── finetune_aug_bg.py       # Fine-tuning script for BiO-CLIP with augmented background
    ├── remove_bg/                   # Directory for background removal scripts
    │   ├──segmentation_scripts/
    │   ├── select_wings_unet.py     # Script for selecting wings using U-Net
    │   └── train_unet256.py         # Training script for U-Net with 256x256 resolution
    ├── download_data.ipynb          # Notebook for downloading dataset
    ├── download_model_weights.ipynb # Notebook for downloading model weights
    └── summary.md                   # Summary document for the project
```
## Reproducing the Results
To reproduce the results presented in this work, follow these steps:

Clone the Repository:

```bash

git clone https://github.com/henryliao85/HDR-anomaly-challenge-2025.git
cd HDR-anomaly-challenge-2025/butterfly\ anomaly\ challenge
```
# Install Dependencies:
Create a virtual environment and install the required packages:

```bash

conda create -n butterfly -c conda-forge python=3.9 -y
conda activate butterfly
pip install -r requirements.txt
Train the U-Net Model:
Run the U-Net training script to obtain the wing segmentation model:
```
```bash

python unet_train.py --train_image_dir path/to/train_images --train_mask_dir path/to/train_masks --val_image_dir path/to/val_images --val_mask_dir path/to/val_masks --epochs 50 --batch_size 16 --lr 1e-4 --output_dir ./output
```
# Augment the Dataset:
Execute the data augmentation script to generate a balanced dataset (with at least 1,000 images per class):

```bash

python data_augmentation.py --input_dir path/to/segmented_wing_images --output_dir path/to/augmented_images
```
# Fine-Tune BiO-CLIP:
Fine-tune the pre-trained BiO-CLIP model and train the classifier head:

```bash

python classifier_train.py --train_image_dir path/to/augmented_images --train_mask_dir path/to/labels --val_image_dir path/to/validation_images --val_mask_dir path/to/validation_labels --epochs 50 --batch_size 16 --lr 1e-4 --output_dir ./output_classifier
```
# Hybrid Probability Estimation:
The hybrid probability function is integrated within the classifier code to compute a scalar probability for hybrid cases. Refer to hybrid_prob.py for details.

By following these steps sequentially—with proper configuration of paths and hyperparameters—you can fully reproduce the methodology and results described in this work.

## References
Imageomics Wing Segmentation:
https://github.com/Imageomics/wing-segmentation
Provides high-quality wing masks generated by YOLO and SAM networks, which are used to train the smaller U-Net model via model distillation.
Additional references for training data and baseline models are listed in the repository's documentation.

