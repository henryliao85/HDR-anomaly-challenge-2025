# Butterfly Anomaly Detection

## Abstract

This document describes an end-to-end pipeline for butterfly subspecies classification. The pipeline comprises the following steps:
1. Downloading and preprocessing data,
2. Training a U-Net model for wing segmentation,
3. Augmenting the wing images to obtain a balanced dataset (with at least 1,000 images per class),
4. Fine-tuning a pre-trained BiO-CLIP model by unfreezing the last two attention blocks (using a small learning rate) and training an additional classifier head (using a larger learning rate) to distinguish butterfly subspecies,
5. Estimating hybrid probabilities by comparing the top two subspecies probabilities.

## Introduction

Butterfly subspecies classification is challenging due to subtle variations in wing patterns and colors. Vision transformers are known to be capable of performing such tasks, but they require careful data preprocessing to focus on the relevant wing patterns rather than background object features. In this work, we propose a full pipeline that leverages segmentation, data augmentation, and modern fine-tuning techniques to build a robust classifier. A key aspect of the pipeline is the estimation of a hybrid probability computed by comparing the top two class probabilities.

## Data Acquisition and Preprocessing

### Downloading the Data

The dataset consists of high-resolution butterfly images along with a CSV metadata file containing subspecies labels and additional attributes. Images are downloaded from a public repository and organized into folders by class.

### Preprocessing

The preprocessing step involves:
- Resizing images to a fixed resolution (e.g., 256×256 pixels),
- Organizing data according to the metadata,
- Ensuring each butterfly subspecies is sufficiently represented.

## Wing Segmentation Using U-Net

### Model Architecture and Training

A U-Net architecture (referred to as `UNet256`) is employed for segmenting butterfly wings from the background. The network follows an encoder-decoder structure with skip connections. Notably, the U-Net was trained using wing masks generated by a combination of YOLO and SAM networks, as detailed in [Imageomics Wing Segmentation](https://github.com/Imageomics/wing-segmentation). In this process, the high-quality segmentation masks produced by the computationally intensive YOLO and SAM models serve as labels for training the much smaller and efficient U-Net. This model distillation approach enables rapid inference while retaining segmentation accuracy.

### Segmentation Process

Once trained, the U-Net model is used to extract the wing regions from butterfly images. The process involves:
- Resizing the input image,
- Applying the U-Net to generate a mask (or using the images segmented by the original YOLO+SAM approach),
- Thresholding the mask to produce a binary output,
- Removing the background based on the binary mask.

## Data Augmentation for Dataset Balancing

After segmentation, the wing images are augmented to ensure each subspecies class contains at least 1,000 images. Augmentation techniques include:
- Random rotations, flips, and affine transformations,
- Color jittering and brightness/contrast adjustments.

Both Albumentations and Torchvision pipelines are used to generate synthetic images, resulting in a balanced dataset crucial for effective model training.

## Fine-Tuning BiO-CLIP for Butterfly Classification

### BiO-CLIP Model Adaptation

We fine-tune a pre-trained BiO-CLIP model to distinguish butterfly subspecies. The adaptation strategy includes:
- Freezing most of the network and unfreezing only the last two attention blocks of the vision transformer,
- Using a small learning rate for the unfrozen layers,
- Training an additional classifier head with a larger learning rate to perform the final classification.

### Training Procedure

The fine-tuning process employs a cross-entropy loss function with the augmented wing images as inputs. A dual learning rate strategy is used:
- A low learning rate for the last two attention blocks,
- A higher learning rate for the classifier head.

This setup allows the model to adapt to the new task while leveraging the pre-trained features.

## Hybrid Probability Estimation

To robustly identify hybrid butterflies, it is insufficient to simply use a metric such as `1 - max(probability)` because in many cases of hybrids the network may assign similarly high probabilities to both parent subspecies. In such situations, relying solely on the maximum probability would lead to misclassification of these ambiguous cases as non-hybrid.

To address this, we compute a hybrid probability that considers the difference between the top two subspecies probabilities. The idea is to capture the model's uncertainty: if the two highest probabilities (corresponding to the parent subspecies) are close in value, it is more likely that the image represents a hybrid. Specifically, the method involves the following steps:
- **Sort** the predicted probabilities for all normal classes (excluding the dedicated hybrid class) in descending order.
- **Treat** the dedicated hybrid class probability (the last class) separately.
- **Define** the hybrid probability as the maximum between the dedicated hybrid probability and 
  \[
  1.0 - (\text{top1} - \text{top2})
  \]
  where \(\text{top1}\) and \(\text{top2}\) are the highest and second-highest probabilities among the normal classes.

This formulation ensures that an image is only classified as non-hybrid when there is a clear distinction between the top predicted subspecies. The following function implements this logic:

```python
def get_hybrid_prob(self, probs):
    """
    Computes the hybrid probability by comparing the top two probabilities of the normal classes.
    
    Logic:
    - Sort all but the last class in descending order.
    - The last class probability (dedicated hybrid) is treated separately.
    - Hybrid probability is defined as the maximum between the last class probability and 
      (1.0 minus the difference between the top two probabilities).
    """
    cl_probs = np.sort(probs[:, :-1], axis=1)[:, ::-1]
    last_cl = probs[:, -1]
    hybrid_probs = np.maximum(last_cl, 1.0 - (cl_probs[:, 0] - cl_probs[:, 1]))
    return hybrid_probs[0]
```

## Repository Structure
The repository is organized as follows:

```graphql

HDR-anomaly-challenge-2025/
└── butterfly anomaly challenge/
    ├── augmentation/
    │   ├── albumentation_augm.py    # Script for data augmentation using Albumentations
    │   └── augment.py               # General data augmentation script
    ├── finetune_bioclip/
    │   ├── data_utils.py            # Utility functions for data handling
    │   ├── dataset.py               # Dataset handling and preprocessing code
    │   └── finetune_aug_bg.py       # Fine-tuning script for BiO-CLIP with augmented, background removed images
    ├── remove_bg/                   # Directory for background removal scripts
    │   ├──segmentation_scripts/
    │   ├── select_wings_unet.py     # Script for selecting wings using U-Net
    │   └── train_unet256.py         # Training script for U-Net with 256x256 resolution
    ├── download_data.ipynb          # Notebook for downloading dataset
    ├── download_model_weights.ipynb # Notebook for downloading model weights
    └── summary.md                   # Summary document for the project
```
## Reproducing the Results
To reproduce the results presented in this work, follow these steps:

Clone the Repository:

```bash

git clone https://github.com/henryliao85/HDR-anomaly-challenge-2025.git
cd HDR-anomaly-challenge-2025/butterfly\ anomaly\ challenge
```
# Download the images:
Use the notebook download_data.ipynb.

# Download the trained model weigts:
Use the notebook download_model_weights.ipynb. In particularly we need the pretraine Unet model (or alternatively one can use the SAM plus YOLO method from the remove_bg/segmentation_scripts and train a Unet using train_unet256.py).



# Use the pretrained U-Net Model:
Run the U-Net to select the wings:

```
```bash

python remove_background.py --model_path path/to/pretrained_unet_model.pth \
                                --csv_path path/to/metadata.csv \
                                --output_folder path/to/output_wing_images
```
# Augment the Dataset:
Execute the data augmentation script to generate a balanced dataset (with at least 1,000 images per class):

```bash

python albumentation_augm.py --orig_img_folder path/to/wing_images \
                      --output_img_folder path/to/output_augmented_images \
                      --csv_path path/to/input.csv \
                      --output_csv_path path/to/output.csv \
                      [--min_images_per_class 1000] [--aug_per_image_high_count 1]
```
# Fine-Tune BiO-CLIP:
Fine-tune the pre-trained BiO-CLIP model and train the classifier head:

```bash

python finetune_aug_bg.py --data_file path/to/data.csv --img_dir path/to/images --clf_save_dir path/to/save_dir [other options...]

```
# Training hyperparameter:
See training_hyperparams.txt for the parameters used for training the model.

# Hybrid Probability Estimation:
The hybrid probability function is integrated within the classifier code to compute a scalar probability for hybrid cases. Refer to model.py for details.



## References
Imageomics Wing Segmentation:
https://github.com/Imageomics/wing-segmentation

BioCLIP model:
https://github.com/Imageomics/BioCLIP

Unet:
https://arxiv.org/abs/1505.04597


