{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Butterfly Classification Pipeline Notebook with Data & Model Downloads\n",
    "\n",
    "This notebook downloads the dataset and model weights, validates them, and then runs the full pipeline:\n",
    "\n",
    "1. Wing segmentation using the pretrained U-Net model\n",
    "2. Data augmentation for dataset balancing\n",
    "3. Fine-tuning the pre-trained BiOâ€‘CLIP classifier\n",
    "\n",
    "Scripts that are designed as command-line tools (e.g. for segmentation, augmentation, and fine-tuning) are invoked using the `!` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "print('Seaborn style set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset CSV\n",
    "\n",
    "Load the butterfly anomaly training CSV from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_url = \"https://raw.githubusercontent.com/Imageomics/HDR-anomaly-challenge/refs/heads/main/files/butterfly_anomaly_train.csv\"\n",
    "df = pd.read_csv(csv_url)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Classification Column\n",
    "\n",
    "For rows missing a direct subspecies label, combine the parent subspecies to form a classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for camid in list(df.loc[df[\"subspecies\"].isna(), \"CAMID\"]):\n",
    "    temp = df.loc[df[\"CAMID\"] == camid]\n",
    "    subspecies = temp[\"parent_subspecies_1\"].astype(str) + \" and \" + temp[\"parent_subspecies_2\"].astype(str)\n",
    "    df.loc[df[\"CAMID\"] == camid, \"classification\"] = subspecies\n",
    "\n",
    "for camid in list(df.loc[df[\"subspecies\"].notna(), \"CAMID\"]):\n",
    "    temp = df.loc[df[\"CAMID\"] == camid]\n",
    "    subspecies = temp[\"subspecies\"].astype(str)\n",
    "    df.loc[df[\"CAMID\"] == camid, \"classification\"] = subspecies\n",
    "\n",
    "print('Classification column added.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Distribution\n",
    "\n",
    "Plot the distribution of images by classification (colored by hybrid status)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df, y=\"classification\", hue=\"hybrid_stat\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Sample Subset for Demo\n",
    "\n",
    "Select a stratified 15% sample of the dataset for a quicker demo download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_set, df_sample = train_test_split(df, test_size=0.15, stratify=df[\"classification\"], random_state=614)\n",
    "print(df_sample.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download Sample Images and Validate\n",
    "\n",
    "Use the functions from the `cautiousrobot` and `sumbuddy` modules to download and validate the sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cautiousrobot.__main__ import download_images\n",
    "from cautiousrobot.buddy_check import BuddyCheck\n",
    "from sumbuddy import get_checksums\n",
    "\n",
    "IMG_DIR = \"sample_images\"\n",
    "CHECKSUM_PATH = \"sample_images_checksums.csv\"\n",
    "\n",
    "print(\"Downloading sample images...\")\n",
    "download_images(\n",
    "    df_sample,\n",
    "    img_dir=IMG_DIR,\n",
    "    log_filepath=\"sample_img_logs.txt\",\n",
    "    error_log_filepath=\"sample_img_error_logs.txt\",\n",
    "    downsample_path=\"sample_images_downsized\",\n",
    "    downsample=256\n",
    ")\n",
    "\n",
    "print(\"Downloading complete. Calculating checksums...\")\n",
    "get_checksums(input_path=IMG_DIR, output_filepath=CHECKSUM_PATH)\n",
    "\n",
    "checksum_df = pd.read_csv(CHECKSUM_PATH, low_memory=False)\n",
    "expected_num_imgs = df_sample.shape[0]\n",
    "print(f\"{checksum_df.shape[0]} images were downloaded to {IMG_DIR} of the {expected_num_imgs} expected.\")\n",
    "\n",
    "buddy_check = BuddyCheck(buddy_id=\"filename\", buddy_col=\"md5\")\n",
    "missing_imgs = buddy_check.validate_download(source_df=df_sample, checksum_df=checksum_df, source_validation_col=\"md5\")\n",
    "if missing_imgs is not None:\n",
    "    missing_imgs.to_csv(\"samples_missing.csv\", index=False)\n",
    "    print(\"See samples_missing.csv for missing image info and check logs.\")\n",
    "else:\n",
    "    print(f\"Buddy check successful. All {expected_num_imgs} expected images accounted for.\")\n",
    "\n",
    "df_sample[\"folder\"] = \"sample_images_downsized\"\n",
    "df_sample.to_csv('./sample_annotation.csv', index=False)\n",
    "print(\"Sample annotation saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download Model Weights\n",
    "\n",
    "Download the required model weights from Huggingface using wget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "file_urls = [\n",
    "    \"https://huggingface.co/pn74870/2025-NSF-HDR-Hackaton-Butterfly-Hybrid-Detection/resolve/main/best_model.pth\",\n",
    "    \"https://huggingface.co/pn74870/2025-NSF-HDR-Hackaton-Butterfly-Hybrid-Detection/resolve/main/cl_head_select_wings.pth\",\n",
    "    \"https://huggingface.co/pn74870/2025-NSF-HDR-Hackaton-Butterfly-Hybrid-Detection/resolve/main/fine_tuned_bioclip_select_wings.pth\"\n",
    "]\n",
    "file_names = [\n",
    "    \"best_unet_model.pth\",\n",
    "    \"cl_head_select_wings.pth\",\n",
    "    \"fine_tuned_bioclip_select_wings.pth\"\n",
    "]\n",
    "\n",
    "for file_url, filename in zip(file_urls, file_names):\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    wget.download(file_url, filename)\n",
    "    print(\"\\nDownload complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Pipeline Steps\n",
    "\n",
    "Call the segmentation, augmentation, and fine-tuning scripts using shell commands. Adjust the paths as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Wing Segmentation\n",
    "!python ../remove_bg/select_wings_unet.py --model_path best_unet_model.pth --csv_path sample_annotation.csv --output_folder ../data/wing_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Data Augmentation\n",
    "!python ../augmentation/albumentation_augm.py --orig_img_folder ../data/wing_images --output_img_folder ../data/augmented_images --csv_path sample_annotation.csv --output_csv_path ../data/augmented_metadata.csv --min_images_per_class 1000 --aug_per_image_high_count 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 Fine-Tuning BiO-CLIP\n",
    "!python ../training/finetune_aug_bg.py --data_file ../data/augmented_metadata.csv --img_dir ../data/augmented_images --clf_save_dir ../models/bioclip_classifier --num_epochs 5 --batch_size 4 --lr_backbone 1e-5 --lr_classifier 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Completed\n",
    "\n",
    "Check the output folders and saved models to verify that each step was executed correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
